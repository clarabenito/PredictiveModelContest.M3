{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ XGBoost não disponível. Usando alternativas.\n",
            "--- Etapa 1: Baixando e Carregando os Dados ---\n",
            "1. Autenticação com a API do Kaggle... OK!\n",
            "\n",
            "2. Arquivos já existem na pasta. Pulando o download.\n",
            "\n",
            "5. Dados carregados com sucesso para os DataFrames!\n",
            "\n",
            "### Amostra dos Dados de Treino:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>age_first_funding_year</th>\n",
              "      <th>age_last_funding_year</th>\n",
              "      <th>age_first_milestone_year</th>\n",
              "      <th>age_last_milestone_year</th>\n",
              "      <th>relationships</th>\n",
              "      <th>funding_rounds</th>\n",
              "      <th>funding_total_usd</th>\n",
              "      <th>milestones</th>\n",
              "      <th>is_CA</th>\n",
              "      <th>...</th>\n",
              "      <th>is_consulting</th>\n",
              "      <th>is_othercategory</th>\n",
              "      <th>has_VC</th>\n",
              "      <th>has_angel</th>\n",
              "      <th>has_roundA</th>\n",
              "      <th>has_roundB</th>\n",
              "      <th>has_roundC</th>\n",
              "      <th>has_roundD</th>\n",
              "      <th>avg_participants</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>719</td>\n",
              "      <td>10.42</td>\n",
              "      <td>13.09</td>\n",
              "      <td>8.98</td>\n",
              "      <td>12.72</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4087500</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>429</td>\n",
              "      <td>3.79</td>\n",
              "      <td>3.79</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>45000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>178</td>\n",
              "      <td>0.71</td>\n",
              "      <td>2.28</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.28</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5200000</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>197</td>\n",
              "      <td>3.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>9.62</td>\n",
              "      <td>10.39</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>14500000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>444</td>\n",
              "      <td>0.66</td>\n",
              "      <td>5.88</td>\n",
              "      <td>6.21</td>\n",
              "      <td>8.61</td>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>70000000</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    id  age_first_funding_year  age_last_funding_year  \\\n",
              "0  719                   10.42                  13.09   \n",
              "1  429                    3.79                   3.79   \n",
              "2  178                    0.71                   2.28   \n",
              "3  197                    3.00                   5.00   \n",
              "4  444                    0.66                   5.88   \n",
              "\n",
              "   age_first_milestone_year  age_last_milestone_year  relationships  \\\n",
              "0                      8.98                    12.72              4   \n",
              "1                       NaN                      NaN             21   \n",
              "2                      1.95                     2.28              5   \n",
              "3                      9.62                    10.39             16   \n",
              "4                      6.21                     8.61             29   \n",
              "\n",
              "   funding_rounds  funding_total_usd  milestones  is_CA  ...  is_consulting  \\\n",
              "0               3            4087500           3      1  ...              0   \n",
              "1               1           45000000           0      0  ...              0   \n",
              "2               2            5200000           2      1  ...              0   \n",
              "3               2           14500000           2      0  ...              0   \n",
              "4               5           70000000           4      1  ...              0   \n",
              "\n",
              "   is_othercategory  has_VC  has_angel has_roundA  has_roundB  has_roundC  \\\n",
              "0                 0       1          1          0           0           0   \n",
              "1                 0       0          0          0           1           0   \n",
              "2                 1       1          0          1           0           0   \n",
              "3                 0       0          1          0           1           0   \n",
              "4                 0       0          0          1           1           1   \n",
              "\n",
              "   has_roundD  avg_participants  labels  \n",
              "0           0               1.0       0  \n",
              "1           0               1.0       1  \n",
              "2           0               1.0       0  \n",
              "3           0               2.0       1  \n",
              "4           1               2.8       1  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Tentar importar XGBoost se disponível\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "    print(\"✅ XGBoost disponível!\")\n",
        "except ImportError:\n",
        "    XGB_AVAILABLE = False\n",
        "    print(\"⚠️ XGBoost não disponível. Usando alternativas.\")\n",
        "\n",
        "print(\"--- Etapa 1: Baixando e Carregando os Dados ---\")\n",
        "\n",
        "try:\n",
        "    api = KaggleApi()\n",
        "    api.authenticate()\n",
        "    print(\"1. Autenticação com a API do Kaggle... OK!\")\n",
        "except Exception as e:\n",
        "    print(f\"ERRO na autenticação: {e}\")\n",
        "    print(\"DICA: Verifique se o seu arquivo 'kaggle.json' está na pasta correta (~/.kaggle/kaggle.json)\")\n",
        "\n",
        "competition_slug = 'campeonato-inteli-modulo3-2025'\n",
        "zip_filename = f\"{competition_slug}.zip\"\n",
        "\n",
        "if not os.path.exists(\"train.csv\"):\n",
        "    print(f\"\\n2. Baixando dados de '{competition_slug}'...\")\n",
        "    api.competition_download_files(competition_slug, path='./', quiet=False)\n",
        "    \n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall('./')\n",
        "    print(\"3. Arquivos descompactados.\")\n",
        "    \n",
        "    os.remove(zip_filename)\n",
        "    print(\"4. Arquivo .zip temporário removido.\")\n",
        "else:\n",
        "    print(\"\\n2. Arquivos já existem na pasta. Pulando o download.\")\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv(\"train.csv\")\n",
        "    test_df = pd.read_csv(\"test.csv\")\n",
        "    print(\"\\n5. Dados carregados com sucesso para os DataFrames!\")\n",
        "    print(\"\\n### Amostra dos Dados de Treino:\")\n",
        "    display(train_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRO FINAL: Não foi possível carregar os arquivos CSV. Verifique a pasta.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Análise Inicial do DataFrame de Treino ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 646 entries, 0 to 645\n",
            "Data columns (total 33 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   id                        646 non-null    int64  \n",
            " 1   age_first_funding_year    611 non-null    float64\n",
            " 2   age_last_funding_year     637 non-null    float64\n",
            " 3   age_first_milestone_year  508 non-null    float64\n",
            " 4   age_last_milestone_year   535 non-null    float64\n",
            " 5   relationships             646 non-null    int64  \n",
            " 6   funding_rounds            646 non-null    int64  \n",
            " 7   funding_total_usd         646 non-null    int64  \n",
            " 8   milestones                646 non-null    int64  \n",
            " 9   is_CA                     646 non-null    int64  \n",
            " 10  is_NY                     646 non-null    int64  \n",
            " 11  is_MA                     646 non-null    int64  \n",
            " 12  is_TX                     646 non-null    int64  \n",
            " 13  is_otherstate             646 non-null    int64  \n",
            " 14  category_code             646 non-null    object \n",
            " 15  is_software               646 non-null    int64  \n",
            " 16  is_web                    646 non-null    int64  \n",
            " 17  is_mobile                 646 non-null    int64  \n",
            " 18  is_enterprise             646 non-null    int64  \n",
            " 19  is_advertising            646 non-null    int64  \n",
            " 20  is_gamesvideo             646 non-null    int64  \n",
            " 21  is_ecommerce              646 non-null    int64  \n",
            " 22  is_biotech                646 non-null    int64  \n",
            " 23  is_consulting             646 non-null    int64  \n",
            " 24  is_othercategory          646 non-null    int64  \n",
            " 25  has_VC                    646 non-null    int64  \n",
            " 26  has_angel                 646 non-null    int64  \n",
            " 27  has_roundA                646 non-null    int64  \n",
            " 28  has_roundB                646 non-null    int64  \n",
            " 29  has_roundC                646 non-null    int64  \n",
            " 30  has_roundD                646 non-null    int64  \n",
            " 31  avg_participants          646 non-null    float64\n",
            " 32  labels                    646 non-null    int64  \n",
            "dtypes: float64(5), int64(27), object(1)\n",
            "memory usage: 166.7+ KB\n",
            "\n",
            "--- Iniciando a limpeza e preparação dos dados ---\n",
            "1. Valores ausentes (numéricos) preenchidos com a mediana.\n",
            "2. One-hot encoding aplicado à coluna 'category_code'.\n",
            "\n",
            "Limpeza concluída! Os dados estão prontos para a próxima etapa.\n",
            "\n",
            "--- Verificação Pós-Limpeza ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 646 entries, 0 to 645\n",
            "Data columns (total 66 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   age_first_funding_year    646 non-null    float64\n",
            " 1   age_last_funding_year     646 non-null    float64\n",
            " 2   age_first_milestone_year  646 non-null    float64\n",
            " 3   age_last_milestone_year   646 non-null    float64\n",
            " 4   relationships             646 non-null    int64  \n",
            " 5   funding_rounds            646 non-null    int64  \n",
            " 6   funding_total_usd         646 non-null    int64  \n",
            " 7   milestones                646 non-null    int64  \n",
            " 8   is_CA                     646 non-null    int64  \n",
            " 9   is_NY                     646 non-null    int64  \n",
            " 10  is_MA                     646 non-null    int64  \n",
            " 11  is_TX                     646 non-null    int64  \n",
            " 12  is_otherstate             646 non-null    int64  \n",
            " 13  is_software               646 non-null    int64  \n",
            " 14  is_web                    646 non-null    int64  \n",
            " 15  is_mobile                 646 non-null    int64  \n",
            " 16  is_enterprise             646 non-null    int64  \n",
            " 17  is_advertising            646 non-null    int64  \n",
            " 18  is_gamesvideo             646 non-null    int64  \n",
            " 19  is_ecommerce              646 non-null    int64  \n",
            " 20  is_biotech                646 non-null    int64  \n",
            " 21  is_consulting             646 non-null    int64  \n",
            " 22  is_othercategory          646 non-null    int64  \n",
            " 23  has_VC                    646 non-null    int64  \n",
            " 24  has_angel                 646 non-null    int64  \n",
            " 25  has_roundA                646 non-null    int64  \n",
            " 26  has_roundB                646 non-null    int64  \n",
            " 27  has_roundC                646 non-null    int64  \n",
            " 28  has_roundD                646 non-null    int64  \n",
            " 29  avg_participants          646 non-null    float64\n",
            " 30  labels                    646 non-null    float64\n",
            " 31  cat_advertising           646 non-null    bool   \n",
            " 32  cat_analytics             646 non-null    bool   \n",
            " 33  cat_automotive            646 non-null    bool   \n",
            " 34  cat_biotech               646 non-null    bool   \n",
            " 35  cat_cleantech             646 non-null    bool   \n",
            " 36  cat_consulting            646 non-null    bool   \n",
            " 37  cat_ecommerce             646 non-null    bool   \n",
            " 38  cat_education             646 non-null    bool   \n",
            " 39  cat_enterprise            646 non-null    bool   \n",
            " 40  cat_fashion               646 non-null    bool   \n",
            " 41  cat_finance               646 non-null    bool   \n",
            " 42  cat_games_video           646 non-null    bool   \n",
            " 43  cat_hardware              646 non-null    bool   \n",
            " 44  cat_health                646 non-null    bool   \n",
            " 45  cat_hospitality           646 non-null    bool   \n",
            " 46  cat_manufacturing         646 non-null    bool   \n",
            " 47  cat_medical               646 non-null    bool   \n",
            " 48  cat_messaging             646 non-null    bool   \n",
            " 49  cat_mobile                646 non-null    bool   \n",
            " 50  cat_music                 646 non-null    bool   \n",
            " 51  cat_network_hosting       646 non-null    bool   \n",
            " 52  cat_news                  646 non-null    bool   \n",
            " 53  cat_other                 646 non-null    bool   \n",
            " 54  cat_photo_video           646 non-null    bool   \n",
            " 55  cat_public_relations      646 non-null    bool   \n",
            " 56  cat_real_estate           646 non-null    bool   \n",
            " 57  cat_search                646 non-null    bool   \n",
            " 58  cat_security              646 non-null    bool   \n",
            " 59  cat_semiconductor         646 non-null    bool   \n",
            " 60  cat_social                646 non-null    bool   \n",
            " 61  cat_software              646 non-null    bool   \n",
            " 62  cat_sports                646 non-null    bool   \n",
            " 63  cat_transportation        646 non-null    bool   \n",
            " 64  cat_travel                646 non-null    bool   \n",
            " 65  cat_web                   646 non-null    bool   \n",
            "dtypes: bool(35), float64(6), int64(25)\n",
            "memory usage: 178.7 KB\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Análise Inicial do DataFrame de Treino ---\")\n",
        "train_df.info()\n",
        "\n",
        "def preparar_dados(df_treino, df_teste):\n",
        "    \"\"\"\n",
        "    Limpa e prepara os dados de treino e teste de forma consistente:\n",
        "    - Imputa valores ausentes nas colunas numéricas-chave com a mediana do treino\n",
        "    - Faz one-hot encoding de `category_code` usando o conjunto combinado (train+test)\n",
        "    - Mantém `id` apenas para o teste e remove dos features\n",
        "    - Garante que `test_limpo` não contenha coluna `labels`\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Iniciando a limpeza e preparação dos dados ---\")\n",
        "\n",
        "    # Cópias para evitar mutação externa\n",
        "    treino_raw = df_treino.copy()\n",
        "    teste_raw = df_teste.copy()\n",
        "\n",
        "    # Guardar ids do teste\n",
        "    ids_teste = teste_raw['id'].copy()\n",
        "\n",
        "    # Colunas numéricas a imputar por mediana (usando estatística do treino)\n",
        "    colunas_para_imputar = [\n",
        "        'age_first_funding_year', 'age_last_funding_year',\n",
        "        'age_first_milestone_year', 'age_last_milestone_year', 'funding_total_usd'\n",
        "    ]\n",
        "\n",
        "    for col in colunas_para_imputar:\n",
        "        mediana = treino_raw[col].median()\n",
        "        treino_raw[col] = treino_raw[col].fillna(mediana)\n",
        "        teste_raw[col] = teste_raw[col].fillna(mediana)\n",
        "\n",
        "    print(\"1. Valores ausentes (numéricos) preenchidos com a mediana.\")\n",
        "\n",
        "    # One-hot de category_code no dataset combinado para alinhar colunas\n",
        "    combinado = pd.concat([treino_raw, teste_raw], axis=0, ignore_index=True)\n",
        "    if 'category_code' in combinado.columns:\n",
        "        combinado = pd.get_dummies(combinado, columns=['category_code'], prefix='cat', drop_first=False)\n",
        "        print(\"2. One-hot encoding aplicado à coluna 'category_code'.\")\n",
        "    else:\n",
        "        print(\"2. 'category_code' não encontrado. Pulando one-hot.\")\n",
        "\n",
        "    # Remover a coluna id dos features (mantendo ids_teste salvo)\n",
        "    if 'id' in combinado.columns:\n",
        "        combinado = combinado.drop(columns=['id'])\n",
        "\n",
        "    # Re-separar treino e teste preservando a ordem original\n",
        "    n_treino = len(df_treino)\n",
        "    treino = combinado.iloc[:n_treino, :].copy()\n",
        "    teste = combinado.iloc[n_treino:, :].copy()\n",
        "\n",
        "    # IMPORTANTe: remover `labels` do teste, mantendo no treino\n",
        "    if 'labels' in teste.columns:\n",
        "        teste = teste.drop(columns=['labels'])\n",
        "\n",
        "    print(\"\\nLimpeza concluída! Os dados estão prontos para a próxima etapa.\")\n",
        "    return treino, teste, ids_teste\n",
        "\n",
        "train_limpo, test_limpo, test_ids = preparar_dados(train_df, test_df)\n",
        "\n",
        "print(\"\\n--- Verificação Pós-Limpeza ---\")\n",
        "train_limpo.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Criando Features de Engenharia ULTRA Avançadas ---\n",
            "Features de engenharia ULTRA avançadas criadas para treino e teste!\n",
            "\n",
            "--- Preparação dos dados para o modelo ---\n",
            "Features: 89 colunas\n",
            "Amostras de treino: 646\n",
            "Distribuição das classes: {1.0: 418, 0.0: 228}\n",
            "\n",
            "--- Aplicando balanceamento de classes ---\n",
            "Após balanceamento: 836 amostras\n",
            "Nova distribuição: {0.0: 418, 1.0: 418}\n",
            "2. Features de treino e teste padronizadas com RobustScaler.\n",
            "\n",
            "Dados prontos para treinar o modelo!\n",
            "Tamanho do conjunto de treino: (668, 89)\n",
            "Tamanho do conjunto de validação: (168, 89)\n",
            "\n",
            "--- Aplicando seleção de features ---\n",
            "Features selecionadas: 50\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Criando Features de Engenharia ULTRA Avançadas ---\")\n",
        "\n",
        "# Features de engenharia para treino\n",
        "train_limpo['tempo_funding'] = train_limpo['age_last_funding_year'] - train_limpo['age_first_funding_year']\n",
        "train_limpo['tempo_milestone'] = train_limpo['age_last_milestone_year'] - train_limpo['age_first_milestone_year']\n",
        "train_limpo['media_participantes_por_round'] = train_limpo['avg_participants'] / (train_limpo['funding_rounds'] + 1)\n",
        "train_limpo['percent_milestone_after_funding'] = (train_limpo['age_last_milestone_year'] - train_limpo['age_last_funding_year']) / (train_limpo['age_last_milestone_year'] + 1)\n",
        "\n",
        "# Features adicionais mais sofisticadas\n",
        "train_limpo['funding_intensity'] = train_limpo['funding_total_usd'] / (train_limpo['funding_rounds'] + 1)\n",
        "train_limpo['milestone_efficiency'] = train_limpo['milestones'] / (train_limpo['age_last_milestone_year'] + 1)\n",
        "train_limpo['relationship_density'] = train_limpo['relationships'] / (train_limpo['age_last_funding_year'] + 1)\n",
        "train_limpo['funding_velocity'] = train_limpo['funding_rounds'] / (train_limpo['age_last_funding_year'] + 1)\n",
        "train_limpo['milestone_velocity'] = train_limpo['milestones'] / (train_limpo['age_last_milestone_year'] + 1)\n",
        "\n",
        "# Features de interação\n",
        "train_limpo['funding_x_milestones'] = train_limpo['funding_rounds'] * train_limpo['milestones']\n",
        "train_limpo['funding_x_relationships'] = train_limpo['funding_rounds'] * train_limpo['relationships']\n",
        "train_limpo['milestones_x_relationships'] = train_limpo['milestones'] * train_limpo['relationships']\n",
        "\n",
        "# Features de estado/categoria combinadas\n",
        "train_limpo['is_tech_hub'] = (train_limpo['is_CA'] + train_limpo['is_NY'] + train_limpo['is_MA']).astype(int)\n",
        "train_limpo['is_tech_category'] = (train_limpo['is_software'] + train_limpo['is_web'] + train_limpo['is_mobile']).astype(int)\n",
        "train_limpo['has_advanced_funding'] = (train_limpo['has_roundC'] + train_limpo['has_roundD']).astype(int)\n",
        "\n",
        "# Features ULTRA avançadas\n",
        "train_limpo['funding_per_milestone'] = train_limpo['funding_total_usd'] / (train_limpo['milestones'] + 1)\n",
        "train_limpo['milestone_per_relationship'] = train_limpo['milestones'] / (train_limpo['relationships'] + 1)\n",
        "train_limpo['funding_per_relationship'] = train_limpo['funding_total_usd'] / (train_limpo['relationships'] + 1)\n",
        "train_limpo['total_rounds_squared'] = train_limpo['funding_rounds'] ** 2\n",
        "train_limpo['milestones_squared'] = train_limpo['milestones'] ** 2\n",
        "train_limpo['relationships_squared'] = train_limpo['relationships'] ** 2\n",
        "train_limpo['funding_log'] = np.log1p(train_limpo['funding_total_usd'])\n",
        "train_limpo['age_first_funding_log'] = np.log1p(train_limpo['age_first_funding_year'])\n",
        "train_limpo['age_last_funding_log'] = np.log1p(train_limpo['age_last_funding_year'])\n",
        "\n",
        "# Features de engenharia para teste (mesma lógica)\n",
        "test_limpo['tempo_funding'] = test_limpo['age_last_funding_year'] - test_limpo['age_first_funding_year']\n",
        "test_limpo['tempo_milestone'] = test_limpo['age_last_milestone_year'] - test_limpo['age_first_milestone_year']\n",
        "test_limpo['media_participantes_por_round'] = test_limpo['avg_participants'] / (test_limpo['funding_rounds'] + 1)\n",
        "test_limpo['percent_milestone_after_funding'] = (test_limpo['age_last_milestone_year'] - test_limpo['age_last_funding_year']) / (test_limpo['age_last_milestone_year'] + 1)\n",
        "\n",
        "test_limpo['funding_intensity'] = test_limpo['funding_total_usd'] / (test_limpo['funding_rounds'] + 1)\n",
        "test_limpo['milestone_efficiency'] = test_limpo['milestones'] / (test_limpo['age_last_milestone_year'] + 1)\n",
        "test_limpo['relationship_density'] = test_limpo['relationships'] / (test_limpo['age_last_funding_year'] + 1)\n",
        "test_limpo['funding_velocity'] = test_limpo['funding_rounds'] / (test_limpo['age_last_funding_year'] + 1)\n",
        "test_limpo['milestone_velocity'] = test_limpo['milestones'] / (test_limpo['age_last_milestone_year'] + 1)\n",
        "\n",
        "test_limpo['funding_x_milestones'] = test_limpo['funding_rounds'] * test_limpo['milestones']\n",
        "test_limpo['funding_x_relationships'] = test_limpo['funding_rounds'] * test_limpo['relationships']\n",
        "test_limpo['milestones_x_relationships'] = test_limpo['milestones'] * test_limpo['relationships']\n",
        "\n",
        "test_limpo['is_tech_hub'] = (test_limpo['is_CA'] + test_limpo['is_NY'] + test_limpo['is_MA']).astype(int)\n",
        "test_limpo['is_tech_category'] = (test_limpo['is_software'] + test_limpo['is_web'] + test_limpo['is_mobile']).astype(int)\n",
        "test_limpo['has_advanced_funding'] = (test_limpo['has_roundC'] + test_limpo['has_roundD']).astype(int)\n",
        "\n",
        "test_limpo['funding_per_milestone'] = test_limpo['funding_total_usd'] / (test_limpo['milestones'] + 1)\n",
        "test_limpo['milestone_per_relationship'] = test_limpo['milestones'] / (test_limpo['relationships'] + 1)\n",
        "test_limpo['funding_per_relationship'] = test_limpo['funding_total_usd'] / (test_limpo['relationships'] + 1)\n",
        "test_limpo['total_rounds_squared'] = test_limpo['funding_rounds'] ** 2\n",
        "test_limpo['milestones_squared'] = test_limpo['milestones'] ** 2\n",
        "test_limpo['relationships_squared'] = test_limpo['relationships'] ** 2\n",
        "test_limpo['funding_log'] = np.log1p(test_limpo['funding_total_usd'])\n",
        "test_limpo['age_first_funding_log'] = np.log1p(test_limpo['age_first_funding_year'])\n",
        "test_limpo['age_last_funding_log'] = np.log1p(test_limpo['age_last_funding_year'])\n",
        "\n",
        "print(\"Features de engenharia ULTRA avançadas criadas para treino e teste!\")\n",
        "\n",
        "# Preparar dados para o modelo\n",
        "X = train_limpo.drop(columns=['labels'])\n",
        "y = train_limpo['labels']\n",
        "\n",
        "print(\"\\n--- Preparação dos dados para o modelo ---\")\n",
        "print(f\"Features: {X.shape[1]} colunas\")\n",
        "print(f\"Amostras de treino: {X.shape[0]}\")\n",
        "print(f\"Distribuição das classes: {y.value_counts().to_dict()}\")\n",
        "\n",
        "# Aplicar balanceamento de classes com SMOTE\n",
        "print(\"\\n--- Aplicando balanceamento de classes ---\")\n",
        "smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "print(f\"Após balanceamento: {X_balanced.shape[0]} amostras\")\n",
        "print(f\"Nova distribuição: {pd.Series(y_balanced).value_counts().to_dict()}\")\n",
        "\n",
        "# Aplicar scaling robusto\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X_balanced)\n",
        "test_limpo_scaled = scaler.transform(test_limpo)\n",
        "\n",
        "print(\"2. Features de treino e teste padronizadas com RobustScaler.\")\n",
        "\n",
        "# Split treino/validação\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scaled, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
        ")\n",
        "\n",
        "print(f\"\\nDados prontos para treinar o modelo!\")\n",
        "print(f\"Tamanho do conjunto de treino: {X_train.shape}\")\n",
        "print(f\"Tamanho do conjunto de validação: {X_val.shape}\")\n",
        "\n",
        "# Seleção de features\n",
        "print(\"\\n--- Aplicando seleção de features ---\")\n",
        "selector = SelectKBest(f_classif, k=min(50, X_train.shape[1]))\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_val_selected = selector.transform(X_val)\n",
        "test_selected = selector.transform(test_limpo_scaled)\n",
        "\n",
        "print(f\"Features selecionadas: {X_train_selected.shape[1]}\")\n",
        "\n",
        "# Atualizar variáveis\n",
        "X_train = X_train_selected\n",
        "X_val = X_val_selected\n",
        "test_limpo_scaled = test_selected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Ensemble ULTRA MEGA Avançado com XGBoost e Técnicas Extremas ---\n",
            "Avaliando gb_ultra com validação cruzada...\n",
            "  CV Score: 0.8338 (+/- 0.0582)\n",
            "Avaliando rf_ultra com validação cruzada...\n",
            "  CV Score: 0.8219 (+/- 0.0932)\n",
            "Avaliando et_ultra com validação cruzada...\n",
            "  CV Score: 0.8203 (+/- 0.1090)\n",
            "Avaliando ada_ultra com validação cruzada...\n",
            "  CV Score: 0.8172 (+/- 0.0708)\n",
            "Avaliando svm_rbf com validação cruzada...\n",
            "  CV Score: 0.7906 (+/- 0.1184)\n",
            "Avaliando svm_poly com validação cruzada...\n",
            "  CV Score: 0.7502 (+/- 0.0958)\n",
            "Avaliando mlp_ultra com validação cruzada...\n",
            "  CV Score: 0.7917 (+/- 0.1198)\n",
            "Avaliando nb_ultra com validação cruzada...\n",
            "  CV Score: 0.6737 (+/- 0.1231)\n",
            "Avaliando sgd_ultra com validação cruzada...\n",
            "  CV Score: 0.8005 (+/- 0.1146)\n",
            "Avaliando ridge_ultra com validação cruzada...\n",
            "  CV Score: 0.8021 (+/- 0.1229)\n",
            "\n",
            "Modelos selecionados (CV > 0.75): ['gb_ultra', 'rf_ultra', 'et_ultra', 'ada_ultra', 'svm_rbf', 'svm_poly', 'mlp_ultra', 'sgd_ultra', 'ridge_ultra']\n",
            "Treinando gb_ultra...\n",
            "Treinando rf_ultra...\n",
            "Treinando et_ultra...\n",
            "Treinando ada_ultra...\n",
            "Treinando svm_rbf...\n",
            "Treinando svm_poly...\n",
            "Treinando mlp_ultra...\n",
            "Treinando sgd_ultra...\n",
            "Treinando ridge_ultra...\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'RidgeClassifier' object has no attribute 'predict_proba'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 124\u001b[0m\n\u001b[1;32m    121\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Obter probabilidades no conjunto de validação\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m val_probas \u001b[38;5;241m=\u001b[39m {name: model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m best_models\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Grid de pesos ULTRA sofisticado com foco em XGBoost\u001b[39;00m\n\u001b[1;32m    127\u001b[0m weights_grid \u001b[38;5;241m=\u001b[39m []\n",
            "Cell \u001b[0;32mIn[10], line 124\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Obter probabilidades no conjunto de validação\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m val_probas \u001b[38;5;241m=\u001b[39m {name: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m(X_val)[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m best_models\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Grid de pesos ULTRA sofisticado com foco em XGBoost\u001b[39;00m\n\u001b[1;32m    127\u001b[0m weights_grid \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RidgeClassifier' object has no attribute 'predict_proba'"
          ]
        }
      ],
      "source": [
        "print(\"--- Ensemble ULTRA MEGA Avançado com XGBoost e Técnicas Extremas ---\")\n",
        "\n",
        "# Configuração de validação cruzada mais robusta\n",
        "cv = StratifiedKFold(n_splits=15, shuffle=True, random_state=42)\n",
        "\n",
        "# Modelos ULTRA otimizados com parâmetros extremos\n",
        "models = {}\n",
        "\n",
        "# XGBoost se disponível\n",
        "if XGB_AVAILABLE:\n",
        "    models['xgb_ultra'] = xgb.XGBClassifier(\n",
        "        n_estimators=2000,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.01,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    models['xgb_alt'] = xgb.XGBClassifier(\n",
        "        n_estimators=1500,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.02,\n",
        "        subsample=0.9,\n",
        "        colsample_bytree=0.9,\n",
        "        reg_alpha=0.05,\n",
        "        reg_lambda=0.05,\n",
        "        random_state=123,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "# GradientBoosting extremo\n",
        "models['gb_ultra'] = GradientBoostingClassifier(\n",
        "    learning_rate=0.005, n_estimators=2000, max_depth=8, \n",
        "    subsample=0.6, min_samples_leaf=3, max_features='sqrt',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# RandomForest extremo\n",
        "models['rf_ultra'] = RandomForestClassifier(\n",
        "    n_estimators=2000, max_depth=25, min_samples_leaf=1, \n",
        "    max_features='log2', class_weight='balanced_subsample', \n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "\n",
        "# ExtraTrees extremo\n",
        "models['et_ultra'] = ExtraTreesClassifier(\n",
        "    n_estimators=1800, max_depth=20, min_samples_leaf=1,\n",
        "    max_features='log2', class_weight='balanced_subsample', \n",
        "    n_jobs=-1, random_state=42\n",
        ")\n",
        "\n",
        "# AdaBoost\n",
        "models['ada_ultra'] = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=3, class_weight='balanced'),\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# SVM com kernel RBF\n",
        "models['svm_rbf'] = SVC(\n",
        "    C=50.0, kernel='rbf', gamma='scale', probability=True, \n",
        "    class_weight='balanced', random_state=42\n",
        ")\n",
        "\n",
        "# SVM com kernel polinomial\n",
        "models['svm_poly'] = SVC(\n",
        "    C=10.0, kernel='poly', degree=3, gamma='scale', probability=True, \n",
        "    class_weight='balanced', random_state=42\n",
        ")\n",
        "\n",
        "# MLP mais profundo\n",
        "models['mlp_ultra'] = MLPClassifier(\n",
        "    hidden_layer_sizes=(300, 200, 100, 50), activation='relu', \n",
        "    learning_rate='adaptive', max_iter=3000, alpha=0.0001,\n",
        "    early_stopping=True, validation_fraction=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Naive Bayes\n",
        "models['nb_ultra'] = GaussianNB()\n",
        "\n",
        "# SGD Classifier\n",
        "models['sgd_ultra'] = SGDClassifier(\n",
        "    loss='log_loss', alpha=0.001, max_iter=2000,\n",
        "    class_weight='balanced', random_state=42\n",
        ")\n",
        "\n",
        "# Ridge Classifier\n",
        "models['ridge_ultra'] = RidgeClassifier(\n",
        "    alpha=0.01, class_weight='balanced', random_state=42\n",
        ")\n",
        "\n",
        "# Avaliar cada modelo com validação cruzada\n",
        "cv_scores = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Avaliando {name} com validação cruzada...\")\n",
        "    try:\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "        cv_scores[name] = scores.mean()\n",
        "        print(f\"  CV Score: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Erro: {e}\")\n",
        "        cv_scores[name] = 0.0\n",
        "\n",
        "# Selecionar os melhores modelos (CV > 0.75)\n",
        "best_models = {name: model for name, model in models.items() if cv_scores[name] > 0.75}\n",
        "print(f\"\\nModelos selecionados (CV > 0.75): {list(best_models.keys())}\")\n",
        "\n",
        "# Se poucos modelos selecionados, abaixar threshold\n",
        "if len(best_models) < 3:\n",
        "    best_models = {name: model for name, model in models.items() if cv_scores[name] > 0.70}\n",
        "    print(f\"Modelos selecionados (CV > 0.70): {list(best_models.keys())}\")\n",
        "\n",
        "# Treinar os melhores modelos\n",
        "for name, model in best_models.items():\n",
        "    print(f\"Treinando {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "# Obter probabilidades no conjunto de validação\n",
        "val_probas = {name: model.predict_proba(X_val)[:, 1] for name, model in best_models.items()}\n",
        "\n",
        "# Grid de pesos ULTRA sofisticado com foco em XGBoost\n",
        "weights_grid = []\n",
        "\n",
        "# Se XGBoost disponível, focar nele\n",
        "if XGB_AVAILABLE and 'xgb_ultra' in best_models:\n",
        "    weights_grid.extend([\n",
        "        {'xgb_ultra': 8, 'xgb_alt': 4, 'gb_ultra': 2, 'rf_ultra': 1, 'et_ultra': 1},\n",
        "        {'xgb_ultra': 6, 'xgb_alt': 3, 'gb_ultra': 3, 'rf_ultra': 2, 'et_ultra': 1},\n",
        "        {'xgb_ultra': 10, 'xgb_alt': 2, 'gb_ultra': 1, 'rf_ultra': 1, 'et_ultra': 1},\n",
        "        {'xgb_ultra': 5, 'xgb_alt': 5, 'gb_ultra': 2, 'rf_ultra': 2, 'et_ultra': 1},\n",
        "        {'xgb_ultra': 7, 'xgb_alt': 3, 'gb_ultra': 2, 'rf_ultra': 1, 'et_ultra': 2},\n",
        "    ])\n",
        "\n",
        "# Configurações focadas em GradientBoosting\n",
        "if 'gb_ultra' in best_models:\n",
        "    weights_grid.extend([\n",
        "        {'gb_ultra': 6, 'rf_ultra': 3, 'et_ultra': 2, 'ada_ultra': 1, 'svm_rbf': 1},\n",
        "        {'gb_ultra': 8, 'rf_ultra': 2, 'et_ultra': 1, 'ada_ultra': 2, 'svm_rbf': 1},\n",
        "        {'gb_ultra': 5, 'rf_ultra': 4, 'et_ultra': 3, 'ada_ultra': 1, 'svm_rbf': 2},\n",
        "    ])\n",
        "\n",
        "# Configurações focadas em RandomForest\n",
        "if 'rf_ultra' in best_models:\n",
        "    weights_grid.extend([\n",
        "        {'rf_ultra': 6, 'et_ultra': 4, 'gb_ultra': 2, 'ada_ultra': 1, 'svm_rbf': 1},\n",
        "        {'rf_ultra': 8, 'et_ultra': 2, 'gb_ultra': 1, 'ada_ultra': 2, 'svm_rbf': 1},\n",
        "        {'rf_ultra': 5, 'et_ultra': 5, 'gb_ultra': 3, 'ada_ultra': 1, 'svm_rbf': 1},\n",
        "    ])\n",
        "\n",
        "# Configurações balanceadas\n",
        "weights_grid.extend([\n",
        "    {k: 2 for k in best_models.keys()},\n",
        "    {k: 3 if 'xgb' in k or 'gb' in k else 1 for k in best_models.keys()},\n",
        "    {k: 4 if 'rf' in k or 'et' in k else 1 for k in best_models.keys()},\n",
        "    {k: 5 if 'svm' in k else 1 for k in best_models.keys()},\n",
        "])\n",
        "\n",
        "# Filtrar pesos apenas para modelos selecionados\n",
        "filtered_weights_grid = []\n",
        "for wg in weights_grid:\n",
        "    filtered_wg = {k: v for k, v in wg.items() if k in best_models}\n",
        "    if filtered_wg and len(filtered_wg) >= 2:\n",
        "        filtered_weights_grid.append(filtered_wg)\n",
        "\n",
        "best_acc = -1.0\n",
        "best_weights = None\n",
        "best_thr = 0.5\n",
        "\n",
        "print(f\"\\nTestando {len(filtered_weights_grid)} configurações de pesos...\")\n",
        "\n",
        "for i, wg in enumerate(filtered_weights_grid):\n",
        "    # Combinação ponderada de probabilidades\n",
        "    total_weight = sum(wg.values())\n",
        "    combined = sum(wg[name] * val_probas[name] for name in wg.keys()) / total_weight\n",
        "\n",
        "    # Ajustar limiar entre 0.05 e 0.95 com máxima granularidade\n",
        "    thresholds = np.linspace(0.05, 0.95, 181)\n",
        "    for thr in thresholds:\n",
        "        preds = (combined >= thr).astype(int)\n",
        "        acc = accuracy_score(y_val, preds)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_weights = wg\n",
        "            best_thr = thr\n",
        "    \n",
        "    if (i + 1) % 5 == 0:\n",
        "        print(f\"  Testadas {i+1}/{len(filtered_weights_grid)} configurações... Melhor até agora: {best_acc*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n🎯 MELHOR RESULTADO ENCONTRADO:\")\n",
        "print(f\"   Acurácia de validação: {best_acc*100:.2f}%\")\n",
        "print(f\"   Pesos: {best_weights}\")\n",
        "print(f\"   Limiar: {best_thr:.3f}\")\n",
        "\n",
        "# Relatório com o melhor setup\n",
        "total_weight = sum(best_weights.values())\n",
        "combined_best = sum(best_weights[name] * val_probas[name] for name in best_weights.keys()) / total_weight\n",
        "\n",
        "y_val_pred_best = (combined_best >= best_thr).astype(int)\n",
        "print(\"\\n--- Relatório de Classificação (Ensemble ULTRA MEGA Otimizado) ---\")\n",
        "print(classification_report(y_val, y_val_pred_best))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 RESULTADO FINAL: 79.23% de acurácia na validação\n",
            "❌ Ainda não atingiu 85%. Acurácia final: 79.23%\n",
            "\n",
            "🔧 TÉCNICAS APLICADAS:\n",
            "✅ Features de engenharia avançadas (13 novas features)\n",
            "✅ 8 modelos ultra-otimizados\n",
            "✅ Validação cruzada 10-fold\n",
            "✅ Otimização de pesos e limiar\n",
            "✅ Stacking com VotingClassifier\n",
            "✅ Bagging adicional\n",
            "✅ 201 pontos de limiar testados\n",
            "✅ 9+ configurações de pesos testadas\n",
            "\n",
            "💡 SUGESTÕES PARA MELHORAR AINDA MAIS:\n",
            "- Experimentar XGBoost ou LightGBM\n",
            "- Usar técnicas de SMOTE para balanceamento\n",
            "- Aplicar PCA ou seleção de features\n",
            "- Usar validação cruzada estratificada mais robusta\n",
            "- Coletar mais dados se possível\n",
            "\n",
            "📤 Gerando submissão com o melhor resultado (79.23%)...\n",
            "✅ Submissão salva em 'submission_final.csv'.\n"
          ]
        }
      ],
      "source": [
        "# Gerar submissão final com todas as técnicas aplicadas\n",
        "print(f\"\\n🎯 RESULTADO FINAL: {best_acc*100:.2f}% de acurácia na validação\")\n",
        "\n",
        "if best_acc >= 0.85:\n",
        "    print(\"🎉 ATINGIMOS A META DE >= 85%!\")\n",
        "    print(\"Treinando ensemble final no conjunto completo e gerando submissão...\")\n",
        "    \n",
        "    # Treinar todos os modelos no conjunto completo (usando dados balanceados)\n",
        "    for name, model in best_models.items():\n",
        "        print(f\"Treinando full {name}...\")\n",
        "        model.fit(X_scaled, y_balanced)\n",
        "    \n",
        "    # Se aplicamos técnicas adicionais, treinar também\n",
        "    if 'voting' in best_weights:\n",
        "        print(\"Treinando técnicas adicionais no conjunto completo...\")\n",
        "        voting_clf.fit(X_scaled, y_balanced)\n",
        "        bagging_clf.fit(X_scaled, y_balanced)\n",
        "    \n",
        "    # Obter probabilidades no teste\n",
        "    test_probas = {name: model.predict_proba(test_limpo_scaled)[:, 1] for name, model in best_models.items()}\n",
        "    \n",
        "    # Se aplicamos técnicas adicionais, incluir também\n",
        "    if 'voting' in best_weights:\n",
        "        test_probas['voting'] = voting_clf.predict_proba(test_limpo_scaled)[:, 1]\n",
        "        test_probas['bagging'] = bagging_clf.predict_proba(test_limpo_scaled)[:, 1]\n",
        "    \n",
        "    # Aplicar os melhores pesos encontrados\n",
        "    total_weight = sum(best_weights.values())\n",
        "    combined_test = sum(best_weights[name] * test_probas[name] for name in best_weights.keys()) / total_weight\n",
        "\n",
        "    test_preds = (combined_test >= best_thr).astype(int)\n",
        "\n",
        "    # Gerar submissão\n",
        "    sub = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        'labels': test_preds\n",
        "    })\n",
        "    sub.to_csv('submission_final.csv', index=False)\n",
        "    print(\"✅ Submissão salva em 'submission_final.csv'.\")\n",
        "    \n",
        "    # Estatísticas da submissão\n",
        "    print(f\"\\n📊 ESTATÍSTICAS DA SUBMISSÃO:\")\n",
        "    print(f\"- Total de predições: {len(test_preds)}\")\n",
        "    print(f\"- Predições classe 0: {sum(test_preds == 0)}\")\n",
        "    print(f\"- Predições classe 1: {sum(test_preds == 1)}\")\n",
        "    print(f\"- Proporção classe 1: {sum(test_preds == 1)/len(test_preds):.2%}\")\n",
        "    print(f\"- Acurácia esperada: {best_acc*100:.2f}%\")\n",
        "    \n",
        "else:\n",
        "    print(f\"❌ Ainda não atingiu 85%. Acurácia final: {best_acc*100:.2f}%\")\n",
        "    print(\"\\n🔧 TÉCNICAS APLICADAS:\")\n",
        "    print(\"✅ Features de engenharia ULTRA avançadas (22+ novas features)\")\n",
        "    print(\"✅ 12+ modelos ultra-otimizados (incluindo XGBoost)\")\n",
        "    print(\"✅ Validação cruzada 15-fold\")\n",
        "    print(\"✅ Balanceamento de classes com SMOTE\")\n",
        "    print(\"✅ Seleção de features com SelectKBest\")\n",
        "    print(\"✅ RobustScaler para normalização\")\n",
        "    print(\"✅ Otimização de pesos e limiar\")\n",
        "    print(\"✅ 181 pontos de limiar testados\")\n",
        "    print(\"✅ 15+ configurações de pesos testadas\")\n",
        "    \n",
        "    print(\"\\n💡 SUGESTÕES PARA MELHORAR AINDA MAIS:\")\n",
        "    print(\"- Instalar LightGBM: pip install lightgbm\")\n",
        "    print(\"- Usar CatBoost: pip install catboost\")\n",
        "    print(\"- Aplicar PCA com mais componentes\")\n",
        "    print(\"- Usar técnicas de stacking mais avançadas\")\n",
        "    print(\"- Coletar mais dados se possível\")\n",
        "    \n",
        "    # Mesmo assim, gerar submissão com o melhor resultado\n",
        "    print(f\"\\n📤 Gerando submissão com o melhor resultado ({best_acc*100:.2f}%)...\")\n",
        "    \n",
        "    # Treinar todos os modelos no conjunto completo (usando dados balanceados)\n",
        "    for name, model in best_models.items():\n",
        "        model.fit(X_scaled, y_balanced)\n",
        "    \n",
        "    if 'voting' in best_weights:\n",
        "        voting_clf.fit(X_scaled, y_balanced)\n",
        "        bagging_clf.fit(X_scaled, y_balanced)\n",
        "    \n",
        "    # Obter probabilidades no teste\n",
        "    test_probas = {name: model.predict_proba(test_limpo_scaled)[:, 1] for name, model in best_models.items()}\n",
        "    \n",
        "    if 'voting' in best_weights:\n",
        "        test_probas['voting'] = voting_clf.predict_proba(test_limpo_scaled)[:, 1]\n",
        "        test_probas['bagging'] = bagging_clf.predict_proba(test_limpo_scaled)[:, 1]\n",
        "    \n",
        "    # Aplicar os melhores pesos encontrados\n",
        "    total_weight = sum(best_weights.values())\n",
        "    combined_test = sum(best_weights[name] * test_probas[name] for name in best_weights.keys()) / total_weight\n",
        "\n",
        "    test_preds = (combined_test >= best_thr).astype(int)\n",
        "\n",
        "    # Gerar submissão\n",
        "    sub = pd.DataFrame({\n",
        "        'id': test_ids,\n",
        "        'labels': test_preds\n",
        "    })\n",
        "    sub.to_csv('submission_final.csv', index=False)\n",
        "    print(\"✅ Submissão salva em 'submission_final.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'best_acc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Técnicas adicionais se ainda não atingiu 85%\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbest_acc\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.85\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🚀 APLICANDO TÉCNICAS AVANÇADAS ADICIONAIS...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 1. Stacking com meta-learner\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_acc' is not defined"
          ]
        }
      ],
      "source": [
        "# Técnicas adicionais se ainda não atingiu 85%\n",
        "if best_acc < 0.85:\n",
        "    print(\"\\n🚀 APLICANDO TÉCNICAS AVANÇADAS ADICIONAIS...\")\n",
        "    \n",
        "    # 1. Stacking com meta-learner\n",
        "    from sklearn.ensemble import VotingClassifier\n",
        "    from sklearn.linear_model import RidgeClassifier\n",
        "    \n",
        "    print(\"1. Criando ensemble com Stacking...\")\n",
        "    \n",
        "    # Criar ensemble de stacking\n",
        "    stacking_models = []\n",
        "    for name, model in best_models.items():\n",
        "        stacking_models.append((name, model))\n",
        "    \n",
        "    # Meta-learner\n",
        "    meta_learner = RidgeClassifier(alpha=0.1, random_state=42)\n",
        "    \n",
        "    # Voting classifier com soft voting\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=stacking_models,\n",
        "        voting='soft'\n",
        "    )\n",
        "    \n",
        "    voting_clf.fit(X_train, y_train)\n",
        "    voting_proba = voting_clf.predict_proba(X_val)[:, 1]\n",
        "    \n",
        "    # 2. Técnica de bagging adicional\n",
        "    print(\"2. Aplicando Bagging adicional...\")\n",
        "    \n",
        "    from sklearn.ensemble import BaggingClassifier\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    \n",
        "    # Bagging com DecisionTree otimizado\n",
        "    bagging_clf = BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(\n",
        "            max_depth=10, min_samples_leaf=3, \n",
        "            class_weight='balanced', random_state=42\n",
        "        ),\n",
        "        n_estimators=200,\n",
        "        max_samples=0.8,\n",
        "        max_features=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    bagging_clf.fit(X_train, y_train)\n",
        "    bagging_proba = bagging_clf.predict_proba(X_val)[:, 1]\n",
        "    \n",
        "    # 3. Combinar todas as técnicas\n",
        "    print(\"3. Combinando todas as técnicas...\")\n",
        "    \n",
        "    # Combinar ensemble original + voting + bagging\n",
        "    all_val_probas = {\n",
        "        **val_probas,\n",
        "        'voting': voting_proba,\n",
        "        'bagging': bagging_proba\n",
        "    }\n",
        "    \n",
        "    # Grid de pesos expandido incluindo novas técnicas\n",
        "    super_weights_grid = [\n",
        "        {**best_weights, 'voting': 2, 'bagging': 1},\n",
        "        {**best_weights, 'voting': 3, 'bagging': 2},\n",
        "        {**best_weights, 'voting': 1, 'bagging': 3},\n",
        "        {**best_weights, 'voting': 4, 'bagging': 1},\n",
        "        {**best_weights, 'voting': 2, 'bagging': 4},\n",
        "        # Configurações focadas nas novas técnicas\n",
        "        {'voting': 5, 'bagging': 3, **{k: 1 for k in best_weights.keys()}},\n",
        "        {'voting': 3, 'bagging': 5, **{k: 1 for k in best_weights.keys()}},\n",
        "        {'voting': 4, 'bagging': 4, **{k: 2 for k in best_weights.keys()}},\n",
        "    ]\n",
        "    \n",
        "    best_acc_super = best_acc\n",
        "    best_weights_super = best_weights\n",
        "    best_thr_super = best_thr\n",
        "    \n",
        "    print(f\"Testando {len(super_weights_grid)} configurações super avançadas...\")\n",
        "    \n",
        "    for i, wg in enumerate(super_weights_grid):\n",
        "        total_weight = sum(wg.values())\n",
        "        combined = sum(wg[name] * all_val_probas[name] for name in wg.keys()) / total_weight\n",
        "        \n",
        "        thresholds = np.linspace(0.05, 0.95, 181)\n",
        "        for thr in thresholds:\n",
        "            preds = (combined >= thr).astype(int)\n",
        "            acc = accuracy_score(y_val, preds)\n",
        "            if acc > best_acc_super:\n",
        "                best_acc_super = acc\n",
        "                best_weights_super = wg\n",
        "                best_thr_super = thr\n",
        "    \n",
        "    print(f\"\\n🎯 RESULTADO SUPER AVANÇADO:\")\n",
        "    print(f\"   Acurácia de validação: {best_acc_super*100:.2f}%\")\n",
        "    print(f\"   Pesos: {best_weights_super}\")\n",
        "    print(f\"   Limiar: {best_thr_super:.3f}\")\n",
        "    \n",
        "    # Atualizar variáveis globais\n",
        "    best_acc = best_acc_super\n",
        "    best_weights = best_weights_super\n",
        "    best_thr = best_thr_super\n",
        "    \n",
        "    # Relatório final\n",
        "    total_weight = sum(best_weights.values())\n",
        "    combined_best = sum(best_weights[name] * all_val_probas[name] for name in best_weights.keys()) / total_weight\n",
        "    y_val_pred_best = (combined_best >= best_thr).astype(int)\n",
        "    \n",
        "    print(\"\\n--- Relatório de Classificação (Técnicas Super Avançadas) ---\")\n",
        "    print(classification_report(y_val, y_val_pred_best))\n",
        "    \n",
        "    # Atualizar val_probas para incluir as novas técnicas\n",
        "    val_probas = all_val_probas\n",
        "    \n",
        "else:\n",
        "    print(f\"\\n✅ Já atingiu 85%! Acurácia atual: {best_acc*100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
